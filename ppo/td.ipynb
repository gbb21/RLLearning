{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pdb\n",
    "import copy\n",
    "from typing import *\n",
    "\n",
    "import collections as cc\n",
    "import sortedcontainers as sc\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "\n",
    "import einops as eo\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch as tc\n",
    "import torch.nn as tcn\n",
    "import torch.nn.functional as tcf\n",
    "import torch.optim as tco\n",
    "import torch.distributions as tcd\n",
    "import einops.layers.torch as eol\n",
    "\n",
    "pd.options.display.max_rows = 40\n",
    "pd.options.display.min_rows = 20\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import gym.spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\tdef __init__(self, _epoch = 0, _debug = True):\n",
    "\t\tself.maxSteps = 20\n",
    "\t\tself.lamb = 0.8\n",
    "\t\tself.epochSize = 40960\n",
    "\t\tself.miniBatchSize = 64\n",
    "\t\tself.delayBatchs = 128\n",
    "\t\tself.totalEpochs = 4\n",
    "\t\tself.testEpochs = 4\n",
    "\t\tself.epoch = _epoch\n",
    "\t\tself.lr = 1E-3\n",
    "\t\tself.targetRewards = 0\n",
    "\t\tself.modelPath = \"./checkpoints/model\"\n",
    "\t\tself.debug = _debug\n",
    "\t\tself.device = tc.device(\"cpu\") if _debug or not tc.cuda.is_available() else tc.device(\"cuda\")\n",
    "\t\tself.mapName = \"4x4\"\n",
    "\t\tprint(self)\n",
    "\t\n",
    "\tdef stepCheckpoint(self, _epoch = None):\n",
    "\t\tif _epoch is not None:\n",
    "\t\t\tself.epoch = _epoch\n",
    "\t\treturn \"{path}_{epoch:02d}.bin\".format(path = self.modelPath, epoch = self.epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env_ = gym.make(\"FrozenLake-v1\", map_name = \"8x8\")\n",
    "env_.render()\n",
    "env_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayEnvironment:\n",
    "\tdef __init__(self, config: Config):\n",
    "\t\tself.name = \"FrozenLake-v1\"\n",
    "\t\tself.instance = gym.make(self.name, map_name = config.mapName)\n",
    "\t\tself.actionSpace = self.instance.action_space.n\n",
    "\t\tself.stateSpace = self.instance.observation_space.n\n",
    "\t\tself.config = config\n",
    "\t\tself.instance.render()\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef createOneHot(data: tc.Tensor, classes: int) -> tc.Tensor:\n",
    "\t\treturn tcf.one_hot(data, classes).requires_grad_(False)\n",
    "\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn \"ReplayEnvironment: \" + self.name\n",
    "\t\n",
    "\t# (newState, reward, done)\n",
    "\tdef step(self, states, actions) -> Tuple[tc.Tensor, tc.Tensor, tc.Tensor]:\n",
    "\t\tdata_ = list()\n",
    "\t\tfor s, a in zip(states, actions):\n",
    "\t\t\tinput_ = self.instance.P[s][a]\n",
    "\t\t\tdata_.append(input_[npr.choice(len(input_), None, True, np.asarray(next(zip(*input_))))][1:])\n",
    "\t\treturn tuple(map(lambda x: tc.tensor(x, requires_grad = False, device = self.config.device), zip(*data_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(tcn.Module):\n",
    "\tdef __init__(self, env: ReplayEnvironment):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.stateSize = env.stateSpace\n",
    "\n",
    "\t\tself.baseModel = tcn.Sequential(\n",
    "\t\t\ttcn.Linear(self.stateSize, self.stateSize),\n",
    "\t\t\ttcn.GELU(),\n",
    "\t\t\ttcn.Linear(self.stateSize, self.stateSize * 2),\n",
    "\t\t\ttcn.GELU(),\n",
    "\t\t\ttcn.Linear(self.stateSize * 2, self.stateSize),\n",
    "\t\t\ttcn.GELU(),\n",
    "\t\t\ttcn.Linear(self.stateSize, 1),\n",
    "\t\t)\n",
    "\t\n",
    "\tdef decayParameters(self):\n",
    "\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" not in x[0], self.named_parameters()))\n",
    "\n",
    "\tdef nondecayParameters(self):\n",
    "\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" in x[0], self.named_parameters()))\n",
    "\t\n",
    "\tdef train(self, _device):\n",
    "\t\treturn self.to(_device)\n",
    "\n",
    "\tdef test(self, _device):\n",
    "\t\treturn self.to(_device)\n",
    "\t\n",
    "\tdef save(self, path):\n",
    "\t\ttc.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load(self, path, device):\n",
    "\t\tself.load_state_dict(tc.load(path, device))\n",
    "\t\n",
    "\tdef forward(self, state: tc.Tensor) -> tc.Tensor:\n",
    "\t\ttemp_ = self.baseModel(tcf.one_hot(state.long(), self.stateSize).float())\n",
    "\t\treturn temp_.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAllStates(model: ActorModel):\n",
    "\tmodel.test(\"cpu\")\n",
    "\ttestState_ = tc.arange(model.stateSize)\n",
    "\tprint(model(testState_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def testTrain(model: ActorModel):\n",
    "\ttestState_ = tc.arange(model.stateSize)\n",
    "\ttestScore_ = tc.zeros(model.stateSize)\n",
    "\ttestScore_[0] = 1\n",
    "\tprint(model(testState_))\n",
    "\tloss_ = tcf.mse_loss(model(testState_), testScore_)\n",
    "\tprint(loss_)\n",
    "\treturn loss_\n",
    "\n",
    "def testTrains():\n",
    "\tconfig_ = Config(0, True)\n",
    "\tmodel_ = ActorModel(ReplayEnvironment(config_))\n",
    "\topt_ = tco.AdamW([{\"params\": model_.decayParameters(), \"weight_decay\": 0.01 }, {\"params\": model_.nondecayParameters()}], config_.lr)\n",
    "\n",
    "\tfor i in range(20):\n",
    "\t\tloss_ = testTrain(model_)\n",
    "\t\topt_.zero_grad()\n",
    "\t\tloss_.backward()\n",
    "\t\topt_.step()\n",
    "\n",
    "testTrains()\n",
    "\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTdUpdateValue(states: Tuple[int], env: ReplayEnvironment, model: ActorModel, config: Config):\n",
    "\twith tc.no_grad():\n",
    "\t\toptions_ = list()\n",
    "\t\tmasks_ = tc.zeros(len(states), dtype = tc.int).requires_grad_(False).to(config.device)\n",
    "\t\tfor a in range(env.actionSpace):\n",
    "\t\t\tnstates_, rewards_, success_ = env.step(states, [a] * len(states))\n",
    "\t\t\tfailed_ = 1 - success_.int()\n",
    "\t\t\tupdated_ = model(nstates_) * failed_ * config.lamb + rewards_ * 10\n",
    "\t\t\tmasks_ |= failed_\n",
    "\n",
    "\t\t\toptions_.append(updated_)\n",
    "\t\tret_ = tc.max(tc.vstack(options_), 0).values\n",
    "\t\t\n",
    "\t\tassert len(ret_) == len(states), \"return dimension doesn't match\"\n",
    "\treturn ret_, masks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(env: ReplayEnvironment, model: ActorModel, config: Config):\n",
    "\tmodel.train(config.device)\n",
    "\topt_ = tco.AdamW([{\"params\": model.decayParameters(), \"weight_decay\": 0.01}, {\"params\": model.nondecayParameters(), \"weight_decay\": 0.0}], config.lr)\n",
    "\t\n",
    "\tlosses_ = list()\n",
    "\tprint(\"begin epoch {0}\".format(config.epoch))\n",
    "\tfor batches_, miniBatch_ in enumerate(zip(*([iter(npr.choice(env.stateSpace, config.epochSize, True))] * config.miniBatchSize))):\n",
    "\t\tassert len(miniBatch_) == config.miniBatchSize, \"minibatch size doesn't match\"\n",
    "\t\topt_.zero_grad()\n",
    "\t\t\n",
    "\t\t# update model:\n",
    "\t\tif batches_ % config.delayBatchs == 0:\n",
    "\t\t\treplayModel_ = copy.deepcopy(model)\n",
    "\t\ttar_, mask_ = computeTdUpdateValue(miniBatch_, env, replayModel_, config)\n",
    "\t\tloss_ = tcf.mse_loss(model(tc.tensor(miniBatch_, device = config.device)) * mask_, tar_ * mask_)\n",
    "\n",
    "\t\tlosses_.append(loss_.item())\n",
    "\t\tloss_.backward()\n",
    "\t\topt_.step()\n",
    "\n",
    "\tprint(\"finish epoch {0}\".format(config.epoch))\n",
    "\treturn losses_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def testRLModel(_env: Environment, _model: ActorModel, _config: Config) -> tc.Tensor:\n",
    "\tstate_ = _env.reset().cpu()\n",
    "\tscores_ = tc.zeros(state_.shape[0]).cpu()\n",
    "\tcompleted_ = tc.zeros(state_.shape[0]).cpu()\n",
    "\t_model = _model.test(tc.device(\"cpu\"))\n",
    "\twith tc.no_grad():\n",
    "\t\tlimits_ = _config.maxSteps\n",
    "\t\twhile completed_.sum() and limits_ > 0:\n",
    "\t\t\tactionDist_, _ = _model(state_.to(_config.device))\n",
    "\t\t\taction_ = actionDist_.sample()\n",
    "\t\t\tstate_, rewards_, completed_ = _env.step(action_)\n",
    "\t\t\tscores_ += rewards_\n",
    "\t\t\tlimits_ -= 1\n",
    "\t\n",
    "\treturn scores_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAll(_env: ReplayEnvironment, _config: Config) -> ActorModel:\n",
    "\tmodel_ = ActorModel(_env)\n",
    "\tepoch_ = _config.epoch\n",
    "\tif epoch_ > 0:\n",
    "\t\tprint(\"loading checkpoint {0}\".format(epoch_))\n",
    "\t\tmodel_.load(_config.stepCheckpoint())\n",
    "\t\n",
    "\tfor t in range(epoch_, _config.totalEpochs):\n",
    "\t\tprint(\"start training epoch {0}\".format(t))\n",
    "\t\t_config.stepCheckpoint(t)\n",
    "\t\tloss_ = trainModel(_env, model_, _config)\n",
    "\t\tprint(\"finish training with {0} loss\".format(np.mean(loss_)))\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tif t % _config.testEpochs == 0:\n",
    "\t\t\tprint(\"start evaluation\")\n",
    "\t\t\treward_ = np.mean(testRLModel(_env, model_, _config))\n",
    "\t\t\tprint(\"finish evaluation with {0} rewards\".format(reward_))\n",
    "\t\t\tprint(\"save model check point\")\n",
    "\t\t\tmodel_.save(_config.stepCheckpoint(t + 1))\n",
    "\t\t\tif _config.targetRewards > 0 and reward_ > _config.targetRewards:\n",
    "\t\t\t\tprint(\"target reward {0} reached, early stop on epoch {1}\".format(_config.targetRewards, t))\n",
    "\t\t\t\tbreak\n",
    "\t\t\"\"\"\n",
    "\t\tprint(\"finish training epoch {0}\".format(t))\n",
    "\t\n",
    "\tprint(\"finish all training steps\")\n",
    "\treturn model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qysh7j5jGqzo"
   },
   "source": [
    "**11. PPO Algorithm: Actor**\n",
    "\n",
    "For `T` steps generate random actions and collect effects in the environment.\n",
    "\n",
    "In the very first moment, consider the current state of the environment and thanks to the current policy evaluates the state, it associate to each action the probability to be best suited to that state.\n",
    "The agent picks from the policy an action with some probability. Then the agent execute that action in the environment, the action modifies the state in the environment and new percepts are collected (data of value function, rewards and probabilities for each actions). Plus, add to collection a vector *masks* that keeps track of the end state of the environment (if the environment has reached `done` state).\n",
    "\n",
    "At then end of this first empirical step,the algorithm computes the generalized advantage estimation to estimate for each action if it improves the value function for the next state (if the action has taken the agent in a better state).\n",
    "\n",
    "Now that all necessary data are collected, the second step of the algorithm analyze and update the weights of the model thanks to the Adam algorithm.\n",
    "\n",
    "At the end of the computation the epoch ends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8JeW3MI0kC3"
   },
   "source": [
    "**12. Implementation of PPO Algorithm: Actor-Critic Style**\n",
    "\n",
    "Main function, for `N` agents (in `N` environments) join parallel *PPO* training.\n",
    "\n",
    "The policy model is a *CNN* with three convolutional and two *256* hidden size dense layers. It implements *Adam* algorithm.\n",
    "\n",
    "It is possible to set transfer learning by setting the `PATH` to the model and `TRANSFER_LEARNING` to `True` in section **4**. \n",
    "\n",
    "`test_rewards` is a list that record previous checkpoints rewards. `train_epoch` record the epoch needed to train the model at the current state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Config object at 0x000001DC36D8AEB0>\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "start training epoch 0\n",
      "begin epoch 0\n",
      "finish epoch 0\n",
      "finish training with 1.4645611109561287 loss\n",
      "finish training epoch 0\n",
      "start training epoch 1\n",
      "begin epoch 1\n",
      "finish epoch 1\n",
      "finish training with 0.6166809556074441 loss\n",
      "finish training epoch 1\n",
      "start training epoch 2\n",
      "begin epoch 2\n",
      "finish epoch 2\n",
      "finish training with 0.4973745384020731 loss\n",
      "finish training epoch 2\n",
      "start training epoch 3\n",
      "begin epoch 3\n",
      "finish epoch 3\n",
      "finish training with 0.4987373967305757 loss\n",
      "finish training epoch 3\n",
      "start training epoch 4\n",
      "begin epoch 4\n",
      "finish epoch 4\n",
      "finish training with 0.4951305897673592 loss\n",
      "finish training epoch 4\n",
      "start training epoch 5\n",
      "begin epoch 5\n",
      "finish epoch 5\n",
      "finish training with 0.4770012002205476 loss\n",
      "finish training epoch 5\n",
      "start training epoch 6\n",
      "begin epoch 6\n",
      "finish epoch 6\n",
      "finish training with 0.49658066583797333 loss\n",
      "finish training epoch 6\n",
      "start training epoch 7\n",
      "begin epoch 7\n",
      "finish epoch 7\n",
      "finish training with 0.4767790062353015 loss\n",
      "finish training epoch 7\n",
      "start training epoch 8\n",
      "begin epoch 8\n",
      "finish epoch 8\n",
      "finish training with 0.5039323419565335 loss\n",
      "finish training epoch 8\n",
      "start training epoch 9\n",
      "begin epoch 9\n",
      "finish epoch 9\n",
      "finish training with 0.5153563828673213 loss\n",
      "finish training epoch 9\n",
      "start training epoch 10\n",
      "begin epoch 10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\tconfig_ = Config(0, False)\n",
    "\tmodel_ = trainAll(ReplayEnvironment(config_), config_)\n",
    "\tmodel_.save(config_.stepCheckpoint(config_.totalEpochs))\n",
    "\t\n",
    "\ttestAllStates(model_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PPO_PONG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
