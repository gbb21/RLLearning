{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import math\n","import random\n","from typing import *\n","\n","import itertools as it\n","\n","import numpy as np\n","import numpy.random as npr\n","import pandas as pd\n","import torch as tc\n","import torch.nn as tcn\n","import torch.nn.functional as tcf\n","import torch.optim as tco\n","import torch.distributions as tcd\n","import einops.layers.torch as eol\n","\n","pd.options.display.max_rows = 40\n","pd.options.display.min_rows = 20\n","pd.options.display.max_columns = 100\n","tc.set_printoptions(edgeitems= 100, linewidth = 160)\n","\n","\n","from IPython.display import display, HTML, clear_output\n","\n","%matplotlib inline\n","\n","import gym\n","import gym.spaces"]},{"cell_type":"markdown","metadata":{},"source":["## Global Configs"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class Config:\n","\tdef __init__(self, _epoch = 0, _debug = True):\n","\t\tself.clip = 0.2\n","\t\tself.c1 = 0.5\n","\t\tself.c2 = 0.01\n","\t\tself.maxSteps = 20\n","\t\tself.gamma = 0.95\n","\t\tself.lamb = 0.9\n","\t\tself.agentCount = 4096\n","\t\tself.miniBatchSize = 256\n","\t\tself.epochIterations = 64\n","\t\tself.totalEpochs = 4\n","\t\tself.testEpochs = 1\n","\t\tself.testAgentCount = 128\n","\t\tself.epoch = _epoch\n","\t\tself.lr = 1E-4\n","\t\tself.targetRewards = 0\n","\t\tself.modelPath = \"./checkpoints/model\"\n","\t\tself.debug = _debug\n","\t\tself.device = tc.device(\"cpu\") if _debug or not tc.cuda.is_available() else tc.device(\"cuda\")\n","\n","\t\tprint(self)\n","\t\n","\tdef stepCheckpoint(self, _epoch = None):\n","\t\tif _epoch is not None:\n","\t\t\tself.epoch = _epoch\n","\t\treturn \"{path}_{epoch:02d}.bin\".format(path = self.modelPath, epoch = self.epoch)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0maUq3510JK7"},"source":["## Generalized Advantage Estimation\n","\n","For each result collected with respect to each action by the algorithm, it computes the delta, which is the sum of current reward and the expected success of the next state (next state value minus current state value), zero if 'done' is reached.\n","So, it computes the sum over all collected states, until last is reached.\n","\n","GAE is computed summing all GAE of previous actions. It is higher if near actions had success, lower otherwhise.\n","\n","At the end sum again the value of current state, so that actions in different states obtain bonus or malus if a better states are reached thanks to those actions."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def calculateGAE(_values: List[tc.Tensor], _rewards: List[tc.Tensor], _masks: List[tc.Tensor], _config: Config) -> tc.Tensor:\n","\tassert 0 < len(_values) == len(_masks) == len(_rewards), \"tensor list size must be greater than 0 and consistent\"\n","\tassert not any(map(lambda x: x.requires_grad, it.chain(_masks, _rewards))), \"masks and rewards should not require grad\"\n","\t\n","\tdecay_ = _config.lamb * _config.gamma\n","\tadvantages_ = list()\n","\tgae_ = tc.zeros_like(_values[0])\n","\tfor t in range(len(_values) - 1, 0, -1):\n","\t\tdelta_ = _rewards[t - 1] + _config.gamma * _values[t] * _masks[t] - _values[t - 1]\n","\t\tgae_ = delta_ + gae_ * decay_ * _masks[t]\n","\t\tadvantages_.append(gae_)\n","\tadvantages_.reverse()\n","\t\n","\treturn advantages_"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v7B8Q3ML0cSj"},"source":[" **10. PPO Algorithm: Critic**\n","\n","For `K` times analyze all the collected data to make gradient updates.\n","\n","At first, the algorithm grabs random mini-batches several times until it covers all data. \n","It takes a state in the batch, from the policy it computes how good is stay in it, then it takes the action that generated that state and computes the new log probability for the action with respect to the current policy (updates during *P* iterations, so, first iteration for new action non visited will be 1).\n","\n","Now, it computes the ratio (`new_prob/old_prob`) of changing of the probability of the action for that state (depending on the training of the policy).\n","\n","The algorithm computes first surrogate function, which promotes the action of which probability variation increased the advantage.\n","After that, it repeats the same computation as before but cutting the ratio values out of a certain range to avoid hysteresis, this is called *CLIP*, and is made to penalize changes to the policy that move the ratio \u000eaway from 1\u000e. It only ignores the change in probability ratio when it would make the objective improve, and it includes the ratio when ratio makes the objective worse.\n","\n","At this point, the algorithm takes the minimum of the two surrogate and do the mean, it computes the 'surrogate loss' in which include only actions that decrease the performances of the actor. \n","It also computes a squared loss for the actor, that is the squared loss of the reward of the action in the state with respect to the policy estimation of the action in the state.\n","\n","Now, the algorithm can compute the loss function, adding the clipped actor loss and the squared critic loss, summing also some entropy bonus, to guarantee the model to promote exploration and not only exploitation.\n","\n","At the end of each step, the algorithm updates the weights of the model thanks to the gradient, generated thanks to Adam algorithm.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"]}],"source":["env_ = gym.make(\"FrozenLake-v1\")\n","env_.render()\n","env_.close()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class Environment:\n","\tdef __init__(self, instanceSize: int):\n","\t\tself.name = \"FrozenLake-v1\"\n","\t\tself.instance = gym.make(self.name)\n","\t\tself.actionSpace = self.instance.action_space\n","\t\tself.stateSpace = self.instance.observation_space\n","\t\tself.trans = self.instance.P\n","\t\n","\tdef reset(self, n: int) -> tc.Tensor:\n","\t\treturn tc.stack([tc.as_tensor([self.instance.reset()]) for i in range(n)]).requires_grad_(False)\n","\n","\tdef randomReset(self, n: int) -> tc.Tensor:\n","\t\treturn tc.randint(self.stateSpace.n, size = (n,)).requires_grad_(False)\n","\tdef __str__(self):\n","\t\treturn \"Environment: \" + self.name\n","\t\n","\tdef step(self, state: tc.Tensor, action: tc.Tensor) -> Tuple[tc.Tensor, tc.Tensor, tc.Tensor]:\n","\t\twith tc.no_grad():\n","\t\t\ttrans_ = [self.trans[s.item()][a.item()] for s, a in zip(state, action)]\n","\t\t\ttransP_ = [npr.choice(len(p), size = None, replace = True, p = list(map(lambda x: x[0], p))) for p in trans_]\n","\t\t\tstates_ = zip(*[t[p][1:] for t, p in zip(trans_, transP_)])\n","\t\t\treturn tc.as_tensor(next(states_), dtype = tc.int, device = state.device), tc.as_tensor(next(states_), dtype = tc.float, device = state.device) * 100, tc.as_tensor(next(states_), dtype = tc.int, device = state.device)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class ActorModel(tcn.Module):\n","\tdef __init__(self, env: Environment):\n","\t\tsuper().__init__()\n","\t\tself.stateSize = env.stateSpace.n\n","\t\tself.actionSize = env.actionSpace.n\n","\n","\t\tself.baseModel = tcn.Sequential(\n","\t\t\ttcn.Linear(self.stateSize, self.stateSize),\n","\t\t\ttcn.GELU(),\n","\t\t\ttcn.Linear(self.stateSize, self.stateSize * 2),\n","\t\t\ttcn.GELU(),\n","\t\t\ttcn.Linear(self.stateSize * 2, self.stateSize * 2),\n","\t\t\ttcn.GELU(),\n","\t\t)\n","\n","\t\tself.actionHead = tcn.Sequential(\n","\t\t\ttcn.Linear(self.stateSize * 2, self.stateSize * 2),\n","\t\t\ttcn.GELU(),\n","\t\t\ttcn.Linear(self.stateSize * 2, self.actionSize, bias = False),\n","\t\t\ttcn.Softmax()\n","\t\t)\n","\n","\t\tself.predHead = tcn.Sequential(\n","\t\t\ttcn.Linear(self.stateSize * 2, self.stateSize),\n","\t\t\ttcn.GELU(),\n","\t\t\ttcn.Linear(self.stateSize, 1)\n","\t\t)\n","\t\n","\tdef decayParameters(self):\n","\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" not in x[0], self.named_parameters()))\n","\n","\tdef nondecayParameters(self):\n","\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" in x[0], self.named_parameters()))\n","\t\n","\tdef save(self, path):\n","\t\ttc.save(self.state_dict(), path)\n","\n","\tdef load(self, path, device):\n","\t\tself.load_state_dict(tc.load(path, device))\n","\t\n","\tdef forward(self, state: tc.Tensor) -> Tuple[tcd.Distribution, tc.Tensor]:\n","\t\ttemp_ = self.baseModel(tcf.one_hot(state.long(), self.stateSize).float())\n","\t\treturn tcd.Categorical(self.actionHead(temp_)), self.predHead(temp_).squeeze(1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class BatchLog:\n","\tdef __init__(self, _states: tc.Tensor, _actions: tc.Tensor, _actionLogProbs: tc.Tensor, _estimatedValues: tc.Tensor, _advantages: tc.Tensor):\n","\t\tassert len(set(map(lambda x: x.shape[0], [_states, _actions, _actionLogProbs, _estimatedValues, _advantages]))) == 1, \"all inputs must have consist size\"\n","\t\tself.states = _states\n","\t\tself.actions = _actions\n","\t\tself.actionLogProbs = _actionLogProbs\n","\t\tself.estimatedValues = _estimatedValues\n","\t\tself.advantages = _advantages\n","\t\n","\tdef generateMiniBatchs(self, _batchSize: int, _dropLast = False) -> Tuple[tc.Tensor, tc.Tensor, tc.Tensor, tc.Tensor, tc.Tensor]:\n","\t\tsize_ = self.states.shape[0]\n","\t\trandIndex_ = tc.randperm(size_)\n","\t\tfor p in range(0, size_, _batchSize):\n","\t\t\tif _dropLast and p + _batchSize > size_:\n","\t\t\t\tbreak\n","\t\t\tbatchIndex_ = randIndex_[p : p + _batchSize]\n","\t\t\tyield self.states[batchIndex_], self.actions[batchIndex_], self.actionLogProbs[batchIndex_], self.estimatedValues[batchIndex_], self.advantages[batchIndex_]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def collectPPOModel(_state: tc.Tensor, _action: tc.Tensor, _oldLogProb: tc.Tensor, _estimatedValue: tc.Tensor, _advantage: tc.Tensor,\n","\t_model: ActorModel, _config: Config) -> tc.Tensor:\n","\t\n","\tactionDist_, value_ = _model(_state)\n","\tnewLogProb_ = actionDist_.log_prob(tc.squeeze(_action))\n","\tratio_ = (newLogProb_ - tc.squeeze(_oldLogProb).detach()).exp() # new_prob/old_prob\n","\tsurr1_ = ratio_ * _advantage.detach()\n","\tsurr2_ = tc.clamp(ratio_, 1.0 - _config.clip, 1.0 + _config.clip) * _advantage.detach()\n","\tactor_loss_ = - tc.min(surr1_, surr2_).mean()\n","\tcritic_loss_ = tcf.mse_loss(value_, _estimatedValue)\n","\tentropy_loss_ = -actionDist_.entropy().mean()\n","\treturn actor_loss_ + _config.c1 * critic_loss_ + _config.c2 * entropy_loss_ "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def testRLModel(_env: Environment, _model: ActorModel, _config: Config) -> tc.Tensor:\n","\tstate_ = _env.reset(_config.testAgentCount)\n","\tscores_ = tc.zeros(state_.shape[0], dtype = tc.float).cpu()\n","\tcompleted_ = tc.zeros(state_.shape[0], dtype = tc.int).cpu()\n","\t_model = _model.to(\"cpu\")\n","\t_model.eval()\n","\t\n","\twith tc.no_grad():\n","\t\tlimits_ = _config.maxSteps\n","\t\twhile (1 - completed_).sum() and limits_ > 0:\n","\t\t\tactionDist_, _ = _model(state_.to(_config.device))\n","\t\t\taction_ = actionDist_.sample()\n","\t\t\tstate_, rewards_, completed_ = _env.step(state_, action_)\n","\t\t\tscores_ += rewards_\n","\t\t\tlimits_ -= 1\n","\t\n","\treturn scores_.tolist()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def standizeAdvantage(advantage: tc.Tensor) -> tc.Tensor:\n","\twith tc.no_grad():\n","\t\treturn (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n","\n","def runRLModel(_env: Environment, _model: ActorModel, _config: Config) -> BatchLog:\n","\tassert _config.maxSteps > 2, \"step size must be greater than 2\"\n","\t\n","\twith tc.no_grad():\n","\t\t_model = _model.to(_config.device)\n","\t\t_model.eval()\n","\t\tstate_ = _env.randomReset(_config.agentCount).to(_config.device)\n","\t\tmask_ = tc.ones(state_.shape[0], dtype = tc.int).to(_config.device)\n","\t\tstates_ = list()\n","\t\tactions_ = list()\n","\t\tactionProbs_ = list()\n","\t\trewards_ = list()\n","\t\tvalues_ = list()\n","\t\tmasks_ = list()\n","\t\tfor _ in range(_config.maxSteps):\n","\t\t\tstates_.append(state_)\n","\t\t\tmasks_.append(mask_)\n","\t\t\tactionDist_, estimatedValue_ = _model(state_)\n","\t\t\tvalues_.append(estimatedValue_)\n","\t\t\taction_ = actionDist_.sample()\n","\t\t\tactions_.append(action_)\n","\t\t\tactionProb_ = actionDist_.log_prob(action_)\n","\t\t\tactionProbs_.append(actionProb_)\n","\t\t\tstate_, reward_, completed_ = _env.step(state_, action_)\n","\t\t\tmask_ = 1 - completed_\n","\t\t\trewards_.append(reward_)\n","\t\t\n","\t\tadvantages_ = tc.cat(calculateGAE(values_, rewards_, masks_, _config))\n","\t\tstates_.pop()\n","\t\tstates_ = tc.cat(states_)\n","\t\tactions_.pop()\n","\t\tactions_ = tc.cat(actions_)\n","\t\tactionProbs_.pop()\n","\t\tactionProbs_ = tc.cat(actionProbs_)\n","\t\tvalues_.pop()\n","\t\tvalues_ = (advantages_ + tc.cat(values_))\n","\t\tadvantages_ = standizeAdvantage(advantages_)\n","\t\tmasks_.pop()\n","\t\tmasks_ = tc.cat(masks_).bool()\n","\t\treturn BatchLog(states_[masks_].detach(), actions_[masks_].detach(), actionProbs_[masks_].detach(), values_[masks_].detach(), advantages_[masks_].detach())"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def trainModel(_env: Environment, _model: ActorModel, _config: Config) -> List[float]:\n","\topt_ = tco.AdamW([{\"params\": _model.decayParameters(), \"weight_decay\": 0.01}, {\"params\": _model.nondecayParameters(), \"weight_decay\": 0.0}], _config.lr)\n","\t_model = _model.to(_config.device)\n","\t_model.train()\n","\t\n","\tfor iter_ in range(_config.epochIterations):\n","\t\tlosses_ = list()\n","\t\tlogs_ = runRLModel(_env, _model, _config)\n","\t\tfor batch_ in logs_.generateMiniBatchs(_config.miniBatchSize):\n","\t\t\topt_.zero_grad()\n","\t\t\tloss_ = collectPPOModel(*batch_, _model, _config)\n","\t\t\tlosses_.append(loss_.item())\n","\t\t\tloss_.backward()\n","\t\t\topt_.step()\n","\t\tprint(\"epoch {0} iteration {1}, loss_ = {2}\".format(_config.epoch, iter_, np.mean(losses_)))\n","\treturn losses_"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def trainAll(_env: Environment, _config: Config) -> ActorModel:\n","\tmodel_ = ActorModel(_env)\n","\tepoch_ = _config.epoch\n","\tif epoch_ > 0:\n","\t\tprint(\"loading checkpoint {0}\".format(epoch_))\n","\t\tmodel_.load(_config.stepCheckpoint())\n","\n","\tfor t in range(epoch_, _config.totalEpochs):\n","\t\tprint(\"start training epoch {0}\".format(t))\n","\t\tloss_ = np.mean(trainModel(_env, model_, _config))\n","\t\tprint(\"finish training with {0} loss\".format(loss_))\n","\n","\t\tif t % _config.testEpochs == 0:\n","\t\t\tprint(\"start evaluation\")\n","\t\t\treward_ = np.mean(testRLModel(_env, model_, _config))\n","\t\t\tprint(\"finish evaluation with {0} rewards\".format(reward_))\n","\t\t\tprint(\"save model check point\")\n","\t\t\tmodel_.save(_config.stepCheckpoint(t + 1))\n","\t\t\tif _config.targetRewards > 0 and reward_ > _config.targetRewards:\n","\t\t\t\tprint(\"target reward {0} reached, early stop on epoch {1}\".format(_config.targetRewards, t))\n","\t\t\t\tbreak\n","\t\n","\t\tprint(\"finish training epoch {0}\".format(t))\n","\t\n","\tprint(\"finish all training steps\")\n","\treturn model_"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qysh7j5jGqzo"},"source":["**11. PPO Algorithm: Actor**\n","\n","For `T` steps generate random actions and collect effects in the environment.\n","\n","In the very first moment, consider the current state of the environment and thanks to the current policy evaluates the state, it associate to each action the probability to be best suited to that state.\n","The agent picks from the policy an action with some probability. Then the agent execute that action in the environment, the action modifies the state in the environment and new percepts are collected (data of value function, rewards and probabilities for each actions). Plus, add to collection a vector *masks* that keeps track of the end state of the environment (if the environment has reached `done` state).\n","\n","At then end of this first empirical step,the algorithm computes the generalized advantage estimation to estimate for each action if it improves the value function for the next state (if the action has taken the agent in a better state).\n","\n","Now that all necessary data are collected, the second step of the algorithm analyze and update the weights of the model thanks to the Adam algorithm.\n","\n","At the end of the computation the epoch ends.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w8JeW3MI0kC3"},"source":["**12. Implementation of PPO Algorithm: Actor-Critic Style**\n","\n","Main function, for `N` agents (in `N` environments) join parallel *PPO* training.\n","\n","The policy model is a *CNN* with three convolutional and two *256* hidden size dense layers. It implements *Adam* algorithm.\n","\n","It is possible to set transfer learning by setting the `PATH` to the model and `TRANSFER_LEARNING` to `True` in section **4**. \n","\n","`test_rewards` is a list that record previous checkpoints rewards. `train_epoch` record the epoch needed to train the model at the current state.\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.Config object at 0x000002683BF24CD0>\n","start training epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["D:\\Users\\gbb21\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0 iteration 0, loss_ = 199.93720334923785\n","epoch 0 iteration 1, loss_ = 208.61294486604888\n","epoch 0 iteration 2, loss_ = 164.57565647516495\n","epoch 0 iteration 3, loss_ = 188.14210349635073\n","epoch 0 iteration 4, loss_ = 242.36613750457764\n","epoch 0 iteration 5, loss_ = 268.42747637960645\n","epoch 0 iteration 6, loss_ = 322.4516826740746\n","epoch 0 iteration 7, loss_ = 358.67720598043854\n","epoch 0 iteration 8, loss_ = 340.9190360945885\n","epoch 0 iteration 9, loss_ = 350.7608364423116\n","epoch 0 iteration 10, loss_ = 324.15786635273633\n","epoch 0 iteration 11, loss_ = 319.3127734375\n","epoch 0 iteration 12, loss_ = 297.9696681213379\n","epoch 0 iteration 13, loss_ = 281.3952464094066\n","epoch 0 iteration 14, loss_ = 270.1421759033203\n","epoch 0 iteration 15, loss_ = 271.7441306993799\n","epoch 0 iteration 16, loss_ = 262.5396153376653\n","epoch 0 iteration 17, loss_ = 251.57675903918695\n","epoch 0 iteration 18, loss_ = 249.2711899097149\n","epoch 0 iteration 19, loss_ = 241.03758066350764\n","epoch 0 iteration 20, loss_ = 245.9880612327939\n","epoch 0 iteration 21, loss_ = 231.90259473568926\n","epoch 0 iteration 22, loss_ = 235.7112685309516\n","epoch 0 iteration 23, loss_ = 243.27132156792038\n","epoch 0 iteration 24, loss_ = 250.66795307124426\n","epoch 0 iteration 25, loss_ = 243.98506388533005\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/2499710960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/2499710960.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0mconfig_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mmodel_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magentCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstepCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotalEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/1329080742.py\u001b[0m in \u001b[0;36mtrainAll\u001b[1;34m(_env, _config)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotalEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start training epoch {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"finish training with {0} loss\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/3890230302.py\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(_env, _model, _config)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miter_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochIterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mlosses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mlogs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunRLModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogs_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerateMiniBatchs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminiBatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                         \u001b[0mopt_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/4281405093.py\u001b[0m in \u001b[0;36mrunRLModel\u001b[1;34m(_env, _model, _config)\u001b[0m\n\u001b[0;32m     26\u001b[0m                         \u001b[0mactionProb_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactionDist_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                         \u001b[0mactionProbs_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactionProb_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                         \u001b[0mstate_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompleted_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                         \u001b[0mmask_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcompleted_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mrewards_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/3718007623.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                         \u001b[0mtrans_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                         \u001b[0mtransP_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrans_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mstates_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransP_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17128/3718007623.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                         \u001b[0mtrans_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                         \u001b[0mtransP_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrans_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mstates_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransP_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def main():\n","\tconfig_ = Config(0, False)\n","\tmodel_ = trainAll(Environment(config_.agentCount), config_)\n","\tmodel_.save(config_.stepCheckpoint(config_.totalEpochs))\n","\n","if __name__ == \"__main__\":\n","\tmain()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"PPO_PONG.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
