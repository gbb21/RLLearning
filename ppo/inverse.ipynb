{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pdb\n",
    "import copy\n",
    "from typing import *\n",
    "\n",
    "import collections as cc\n",
    "import sortedcontainers as sc\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "\n",
    "import einops as eo\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch as tc\n",
    "import torch.nn as tcn\n",
    "import torch.nn.functional as tcf\n",
    "import torch.optim as tco\n",
    "import torch.distributions as tcd\n",
    "import einops.layers.torch as eol\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "pd.options.display.max_rows = 40\n",
    "pd.options.display.min_rows = 20\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import gym.spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\tdef __init__(self, _epoch = 0, _debug = True):\n",
    "\t\tself.maxSteps = 24\n",
    "\t\tself.lamb = 0.8\n",
    "\t\tself.miniBatchSize = 64\n",
    "\t\tself.delayBatchs = 128\n",
    "\t\tself.totalEpochs = 4\n",
    "\t\tself.testEpochs = 4\n",
    "\t\tself.initEpochs = 10\n",
    "\t\tself.epoch = _epoch\n",
    "\t\tself.lr = 1E-3\n",
    "\t\tself.targetRewards = 0\n",
    "\t\tself.modelPath = \"./checkpoints/model\"\n",
    "\t\tself.debug = _debug\n",
    "\t\tself.device = tc.device(\"cpu\") if _debug or not tc.cuda.is_available() else tc.device(\"cuda\")\n",
    "\t\tself.mapName = \"4x4\"\n",
    "\t\tself.demoFinalRewards = 1\n",
    "\t\tself.demoRewardDecay = 0.9\n",
    "\t\tself.stateEstimationWeight = 0.5\n",
    "\t\tself.trainingRandomInitState = True\n",
    "\t\tprint(self)\n",
    "\t\n",
    "\tdef stepCheckpoint(self, _epoch = None):\n",
    "\t\tif _epoch is not None:\n",
    "\t\t\tself.epoch = _epoch\n",
    "\t\treturn \"{path}_{epoch:02d}.bin\".format(path = self.modelPath, epoch = self.epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayEnvironment:\n",
    "\tdef __init__(self, config: Config):\n",
    "\t\tself.name = \"FrozenLake-v1\"\n",
    "\t\tself.instance = gym.make(self.name, map_name = config.mapName)\n",
    "\t\tself.actionSpace = self.instance.action_space.n\n",
    "\t\tself.stateSpace = self.instance.observation_space.n\n",
    "\t\tself.config = config\n",
    "\t\tself.transProbs, self.transStates, self.transRewards, self.transEnds = self._convertTransMat(3, self.instance.P)\n",
    "\t\tself.instance.render()\n",
    "\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn \"ReplayEnvironment: \" + self.name\n",
    "\t\n",
    "\t# (prob, nstate, rewards, end)\n",
    "\t@tc.no_grad()\n",
    "\tdef _convertTransMat(self, probSpace: int, transDict: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]]) -> Tuple[tc.FloatTensor, tc.IntTensor, tc.FloatTensor, tc.BoolTensor]:\n",
    "\t\tself.probSpace = probSpace\n",
    "\t\tweights_ = tc.zeros((self.stateSpace, self.actionSpace, probSpace), dtype = tc.float)\n",
    "\t\trewards_ = tc.zeros_like(weights_)\n",
    "\t\tnstates_ = tc.zeros((self.stateSpace, self.actionSpace, probSpace), dtype = tc.int)\n",
    "\t\tends_ = tc.zeros((self.stateSpace, self.actionSpace, probSpace), dtype = tc.bool)\n",
    "\n",
    "\t\tfor k1, s1 in transDict.items():\n",
    "\t\t\tfor k2, s2 in s1.items():\n",
    "\t\t\t\tweights_[k1, k2] = tc.as_tensor([p[0] for p in it.islice(s2, probSpace)], dtype = tc.float)\n",
    "\t\t\t\trewards_[k1, k2] = tc.as_tensor([p[2] for p in it.islice(s2, probSpace)], dtype = tc.float)\n",
    "\t\t\t\tnstates_[k1, k2] = tc.as_tensor([p[1] for p in it.islice(s2, probSpace)], dtype = tc.int)\n",
    "\t\t\t\tends_[k1, k2] = tc.as_tensor([p[3] for p in it.islice(s2, probSpace)], dtype = tc.bool)\n",
    "\t\t\n",
    "\t\treturn weights_, nstates_, rewards_, ends_\n",
    "\t\n",
    "\t# (newState, reward, done)\n",
    "\t@tc.no_grad()\n",
    "\tdef step(self, states: tc.IntTensor, actions: tc.IntTensor) -> Tuple[tc.Tensor, tc.Tensor, tc.Tensor]:\n",
    "\t\tactions = actions.unsqueeze(1).unsqueeze(2).expand((-1, -1, self.probSpace))\n",
    "\t\t\n",
    "\t\tweights_ = self.transProbs[states].take_along_dim(actions, dim = 1).squeeze(dim = 1)\n",
    "\t\trewards_ = self.transRewards[states].take_along_dim(actions, dim = 1).squeeze(dim = 1)\n",
    "\t\tnstates_ = self.transStates[states].take_along_dim(actions, dim = 1).squeeze(dim = 1)\n",
    "\t\tends_ = self.transEnds[states].take_along_dim(actions, dim = 1).squeeze(dim = 1)\n",
    "\n",
    "\t\tidxes_ = tc.multinomial(weights_, 1, True)\n",
    "\t\treturn nstates_.take_along_dim(idxes_, dim = 1).squeeze(dim = 1), rewards_.take_along_dim(idxes_, dim = 1).squeeze(dim = 1), ends_.take_along_dim(idxes_, dim = 1).squeeze(dim = 1)\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(tcn.Module):\n",
    "\tdef __init__(self, env: ReplayEnvironment):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tstateSize_ = env.stateSpace\n",
    "\t\tactionSize_ = env.actionSpace\n",
    "\n",
    "\t\tself.baseModel = tcn.Sequential(\n",
    "\t\t\ttcn.Linear(stateSize_, stateSize_ * 2),\n",
    "\t\t\ttcn.GELU(),\n",
    "\t\t\ttcn.Linear(stateSize_ * 2, stateSize_ * 2),\n",
    "\t\t\ttcn.GELU()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.actionHead = tcn.Sequential(\n",
    "\t\t\ttcn.Linear(stateSize_ * 2, stateSize_ * 2),\n",
    "\t\t\ttcn.GELU(),\n",
    "\t\t\ttcn.Linear(stateSize_ * 2, actionSize_),\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.valueHead = tcn.Linear(stateSize_ * 2, 1)\n",
    "\t\n",
    "\tdef decayParameters(self):\n",
    "\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" not in x[0], self.named_parameters()))\n",
    "\n",
    "\tdef nondecayParameters(self):\n",
    "\t\treturn map(lambda x: x[1], filter(lambda x: \"bias\" in x[0], self.named_parameters()))\n",
    "\t\n",
    "\tdef save(self, path):\n",
    "\t\ttc.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load(self, path):\n",
    "\t\tself.load_state_dict(tc.load(path))\n",
    "\n",
    "\tdef forward(self, states: tc.Tensor, invtemp: float = 1.0) -> tc.Tensor:\n",
    "\t\tbaseOutput_ = self.baseModel(tcf.one_hot(states.long(), self.env.stateSpace).float())\n",
    "\t\tlogits_ = self.actionHead(baseOutput_)\n",
    "\t\tactionDist_ = tcd.Categorical(tcf.softmax(logits_ * invtemp, dim = 1))\n",
    "\t\tstateValue_ = self.valueHead(baseOutput_).squeeze()\n",
    "\t\treturn actionDist_, stateValue_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializePolicy(model: ActorModel, env: ReplayEnvironment, config: Config):\n",
    "\topt_ = tco.AdamW([{\"params\": model.decayParameters(), \"weight_decay\": 0.01}, {\"params\": model.nondecayParameters(), \"weight_decay\": 0.0}], config.lr)\n",
    "\tstates_ = tc.arange(0, env.stateSpace, 1, dtype = tc.int)\n",
    "\tmodel.eval()\n",
    "\t_, target_ = model(states_)\n",
    "\ttarget_ -= target_.min(keepdim = False)\n",
    "\ttarget_ /= target_.max(keepdim = False)\n",
    "\ttarget_.requires_grad_(False)\n",
    "\n",
    "\tmodel.train()\n",
    "\tfor i in range(config.initEpochs):\n",
    "\t\tactionDists_, preds_ = model(states_)\n",
    "\t\tloss_ = tcf.mse_loss(preds_, target_)\n",
    "\t\topt_.zero_grad()\n",
    "\t\tloss_.backward()\n",
    "\t\topt_.step()\n",
    "\tprint(\"initialization loss: {0}\".format(loss_.item()))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns [states, values, actions, advantages]\n",
    "def generatePaths(model: ActorModel, env: ReplayEnvironment, config: Config):\n",
    "\tmodel.eval()\n",
    "\n",
    "\tbatchSize_ = config.miniBatchSize * config.delayBatchs\n",
    "\n",
    "\tif config.trainingRandomInitState:\n",
    "\t\tstate_ = tc.randint(env.stateSpace, batchSize_)\n",
    "\telse:\n",
    "\t\tstate_ = tc.zeros(batchSize_, dtype = tc.int)\n",
    "\t\n",
    "\tend_ = tc.ones(batchSize_, tc.int)\n",
    "\t\n",
    "\tstates_ = list()\n",
    "\tvalues_ = list()\n",
    "\tadvantages_ = list()\n",
    "\tactions_ = list()\n",
    "\tmasks_ = list()\n",
    "\tvalue_ = None\n",
    "\tadv_ = None\n",
    "\n",
    "\twith tc.no_grad():\n",
    "\t\tfor s in range(config.maxSteps):\n",
    "\t\t\tstates_.append(state_)\n",
    "\t\t\tmasks_.append(end_)\n",
    "\t\t\tactDists_, estims_ = model(state_)\n",
    "\t\t\tacts_ = actDists_.sample()\n",
    "\t\t\tactions_.append(acts_.detach())\n",
    "\t\t\tdecayedEstims_ = estims_ * config.lamb\n",
    "\t\t\tif value_ is not None:\n",
    "\t\t\t\tvalue_ += decayedEstims_\n",
    "\t\t\tif adv_ is not None:\n",
    "\t\t\t\tadv_ += decayedEstims_\n",
    "\t\t\t\n",
    "\t\t\tstate_, value_, end_ = env.step(state_, acts_)\n",
    "\t\t\tend_ = 1 - end_\n",
    "\t\t\t# generate \"partial\" value and advantage for the current\n",
    "\t\t\tvalues_.append(value_)\n",
    "\t\t\tadv_ = value_ - estims_\n",
    "\t\t\tadvantages_.append(adv_)\n",
    "\n",
    "\t\testims_ = model(state_)[1] * config.lamb\n",
    "\t\tvalue_ += estims_\n",
    "\t\tadv_ += estims_\n",
    "\t\t\n",
    "\t\tidx_ = tc.cat(masks_).to(tc.bool)\n",
    "\t\treturn TensorDataset(tc.cat(states_)[idx_].detach(), tc.cat(values_)[idx_].detach(), tc.cat(actions_)[idx_].detach(), tc.cat(advantages_)[idx_].detach())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(env: ReplayEnvironment, model: ActorModel, config: Config):\n",
    "\topt_ = tco.AdamW([{\"params\": model.decayParameters(), \"weight_decay\": 0.01}, {\"params\": model.nondecayParameters(), \"weight_decay\": 0.0}], config.lr)\n",
    "\tprint(\"begin epoch {0}, path generation... \".format(config.epoch))\n",
    "\t\n",
    "\tdataloader_ = DataLoader(generatePaths(model, env, config), config.miniBatchSize, shuffle = True)\n",
    "\tprint(\"begin epoch {0}, training... \".format(config.epoch))\n",
    "\t\n",
    "\tmodel.train()\n",
    "\tlosses_ = list()\n",
    "\tfor batches_, miniBatch_ in enumerate(dataloader_):\n",
    "\t\tassert len(miniBatch_) == config.miniBatchSize, \"minibatch size doesn't match\"\n",
    "\t\t\n",
    "\t\tstates_, values_, actions_, advantages_ = miniBatch_\n",
    "\n",
    "\t\tactionDists_, pvals_ = model(states_)\n",
    "\t\t\n",
    "\t\tloss1_ = -tc.mean(actionDists_.log_prob(actions_) * advantages_)\n",
    "\t\tloss2_ = tcf.mse_loss(pvals_, values_)\n",
    "\t\tloss_ = loss1_ + loss2_ * config.stateEstimationWeight\n",
    "\n",
    "\t\topt_.zero_grad()\n",
    "\t\tloss_.backward()\n",
    "\t\topt_.step()\n",
    "\t\t\n",
    "\t\tlosses_.append(loss_.item())\n",
    "\n",
    "\tprint(\"finish epoch {0} with mean loss {1}\".format(config.epoch, np.mean(losses_)))\n",
    "\treturn losses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAll(env: ReplayEnvironment, config: Config) -> ActorModel:\n",
    "\tmodel_ = ActorModel(env)\n",
    "\tepoch_ = config.epoch\n",
    "\t\n",
    "\tif epoch_ > 0:\n",
    "\t\tprint(\"loading checkpoint {0}\".format(epoch_))\n",
    "\t\tmodel_.load(config.stepCheckpoint())\n",
    "\telse:\n",
    "\t\tinitializePolicy(model_, env, config)\n",
    "\t\n",
    "\tfor t in range(epoch_, config.totalEpochs):\n",
    "\t\tprint(\"start training epoch {0}\".format(t))\n",
    "\t\tpath_ = config.stepCheckpoint(t)\n",
    "\t\tloss_ = trainModel(env, model_, config)\n",
    "\t\tprint(\"finish training, save to path {0}\".format(path_))\n",
    "\t\ttc.save(model_.state_dict(), path_)\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tif t % _config.testEpochs == 0:\n",
    "\t\t\tprint(\"start evaluation\")\n",
    "\t\t\treward_ = np.mean(testRLModel(_env, model_, _config))\n",
    "\t\t\tprint(\"finish evaluation with {0} rewards\".format(reward_))\n",
    "\t\t\tprint(\"save model check point\")\n",
    "\t\t\tmodel_.save(_config.stepCheckpoint(t + 1))\n",
    "\t\t\tif _config.targetRewards > 0 and reward_ > _config.targetRewards:\n",
    "\t\t\t\tprint(\"target reward {0} reached, early stop on epoch {1}\".format(_config.targetRewards, t))\n",
    "\t\t\t\tbreak\n",
    "\t\t\"\"\"\n",
    "\t\tprint(\"finish training epoch {0}\".format(t))\n",
    "\t\n",
    "\tprint(\"finish all training steps\")\n",
    "\treturn model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Config object at 0x000001DC36D8AEB0>\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "start training epoch 0\n",
      "begin epoch 0\n",
      "finish epoch 0\n",
      "finish training with 1.4645611109561287 loss\n",
      "finish training epoch 0\n",
      "start training epoch 1\n",
      "begin epoch 1\n",
      "finish epoch 1\n",
      "finish training with 0.6166809556074441 loss\n",
      "finish training epoch 1\n",
      "start training epoch 2\n",
      "begin epoch 2\n",
      "finish epoch 2\n",
      "finish training with 0.4973745384020731 loss\n",
      "finish training epoch 2\n",
      "start training epoch 3\n",
      "begin epoch 3\n",
      "finish epoch 3\n",
      "finish training with 0.4987373967305757 loss\n",
      "finish training epoch 3\n",
      "start training epoch 4\n",
      "begin epoch 4\n",
      "finish epoch 4\n",
      "finish training with 0.4951305897673592 loss\n",
      "finish training epoch 4\n",
      "start training epoch 5\n",
      "begin epoch 5\n",
      "finish epoch 5\n",
      "finish training with 0.4770012002205476 loss\n",
      "finish training epoch 5\n",
      "start training epoch 6\n",
      "begin epoch 6\n",
      "finish epoch 6\n",
      "finish training with 0.49658066583797333 loss\n",
      "finish training epoch 6\n",
      "start training epoch 7\n",
      "begin epoch 7\n",
      "finish epoch 7\n",
      "finish training with 0.4767790062353015 loss\n",
      "finish training epoch 7\n",
      "start training epoch 8\n",
      "begin epoch 8\n",
      "finish epoch 8\n",
      "finish training with 0.5039323419565335 loss\n",
      "finish training epoch 8\n",
      "start training epoch 9\n",
      "begin epoch 9\n",
      "finish epoch 9\n",
      "finish training with 0.5153563828673213 loss\n",
      "finish training epoch 9\n",
      "start training epoch 10\n",
      "begin epoch 10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\tconfig_ = Config(0, False)\n",
    "\tmodel_ = trainAll(ReplayEnvironment(config_), config_)\n",
    "\t\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PPO_PONG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cb1ba66c70c9f1b4bc3712c3a7c9a4b0257feab7a6813209c9ff0ab78fb2ef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
